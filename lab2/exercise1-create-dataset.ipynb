{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x4HI2mpwlrcn"
   },
   "source": [
    "# Lab 2: Data Engineering\n",
    "## Exercise 1: Create a dataset\n",
    "\n",
    "Until now, we have used the CIFAR-10 dataset from Keras. Every script started by preprocessing this dataset into a subset with the four classes that we chose. Because CIFAR-10 is quite small, this doesn't take much time, so it didn't bother us.\n",
    "\n",
    "Imagine you have a gigantic dataset. This is quite normal when you are doing machine learning in the real world. Preprocessing it takes a long time, so we really don't want to do it for every training experiment. What's the solution? ClearML Datasets!\n",
    "\n",
    "Let's create a dataset in ClearML for our subset of CIFAR-10. This is done by running a script to preprocess the data, write it to the file system, and then upload it to ClearML. We need to write data to files first because ClearML datasets are always created from files. You can write any Python data to files using `pickle.dump()`, or when you have NumPy arrays, use `numpy.save()`.\n",
    "\n",
    "To create a ClearML dataset use `Dataset.create()`:\n",
    "\n",
    "https://clear.ml/docs/latest/docs/references/sdk/dataset#datasetcreate \\\n",
    "https://clear.ml/docs/latest/docs/clearml_data/data_management_examples/data_man_python\n",
    "\n",
    "Afterward, go to the ClearML website, click on DATASETS on the left side, and find the dataset you just created.\n",
    "\n",
    "We won't do this now, but later we can use [Dataset.get()](https://clear.ml/docs/latest/docs/references/sdk/dataset/#datasetget) to download our new dataset before training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q clearml nbconvert\n",
    "\n",
    "%env CLEARML_WEB_HOST=https://app.clear.ml\n",
    "%env CLEARML_API_HOST=https://api.clear.ml\n",
    "%env CLEARML_FILES_HOST=https://files.clear.ml\n",
    "%env CLEARML_API_ACCESS_KEY=<your key here>\n",
    "%env CLEARML_API_SECRET_KEY=<your key here>\n",
    "# TODO fill in your keys    ^^^^^^^^^^^^^^^\n",
    "\n",
    "import tensorflow.keras as keras\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from clearml import Dataset\n",
    "\n",
    "# download the dataset\n",
    "(images, labels), _ = keras.datasets.cifar10.load_data()\n",
    "\n",
    "# there are 10 classes of images\n",
    "all_classes = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
    "               'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "# choose four classes (feel free to change this!)\n",
    "class_names = [\"bird\", \"cat\", \"deer\", \"dog\"]\n",
    "print(\"Class names:\", class_names)\n",
    "\n",
    "# only keep images of these classes\n",
    "class_indexes = [all_classes.index(c) for c in class_names]\n",
    "to_keep = np.array([l in class_indexes for l in labels])\n",
    "images = images[to_keep]\n",
    "labels = labels[to_keep]\n",
    "\n",
    "# change indexes from 10 to 2 classes\n",
    "labels = np.array([class_indexes.index(l) for l in labels])\n",
    "\n",
    "# normalize pixels between 0 and 1\n",
    "images = images / 255.0\n",
    "\n",
    "# ---\n",
    "# TMP Only store 10 images/labels, because we want fast upload.\n",
    "# This way we can learn the dataset SDK without waiting too long.\n",
    "images = images[:10]\n",
    "labels = labels[:10]\n",
    "# ---\n",
    "\n",
    "# split into train and test set\n",
    "split = round(len(images) * 0.8)\n",
    "train_images = images[:split]\n",
    "train_labels = labels[:split]\n",
    "test_images = images[split:]\n",
    "test_labels = labels[split:]\n",
    "print(\"Number of train images:\", len(train_images))\n",
    "print(\"Number of test images:\", len(test_images))\n",
    "\n",
    "# TODO save numpy arrays to disk\n",
    "np.save(...)\n",
    "np.save(...)\n",
    "np.save(...)\n",
    "np.save(...)\n",
    "\n",
    "# TODO create ClearML dataset\n",
    "dataset = Dataset.create(...)\n",
    "dataset.add_files(...)\n",
    "dataset.add_files(...)\n",
    "dataset.add_files(...)\n",
    "dataset.add_files(...)\n",
    "\n",
    "dataset.upload()\n",
    "dataset.finalize()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "cnn.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
